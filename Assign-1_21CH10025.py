# -*- coding: utf-8 -*-
"""1st.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17wv84cL7A-zX2tOh76iZxSkB0Zg3sfnu
"""

# Name - Gaurav Kumar
# Roll No- 21CH10025.
# Assign. No- 01;

"""### Implementation"""

class Perceptron:
    def __init__(self, num_features):
        self.weights = np.zeros(num_features)
        self.bias = 0

    def predict(self, X):
        return np.sign(np.dot(X, self.weights) + self.bias)

    def train(self, X, y, max_epochs=2000):
        for epoch in range(max_epochs):
            misclassified = False
            for i in range(len(X)):
                prediction = self.predict(X[i])
                if prediction != y[i]:
                    self.weights += y[i] * X[i]
                    self.bias += y[i]
                    misclassified = True
            if not misclassified:
                #print(f"Converged in {epoch+1} epochs.")
                break;

def k_fold_cross_validation(X, y, k):
    fold_size = len(X) // k           # floor division;
    indices = np.arange(len(X))
    np.random.shuffle(indices)

    for fold in range(k):
        # Seperating indices for training and validation using slicing of array;
        validation_indices = indices[fold * fold_size: (fold + 1) * fold_size]
        train_indices = np.concatenate([indices[:fold * fold_size], indices[(fold + 1) * fold_size:]])

        # Run the model ;
        X_train, y_train = X[train_indices], y[train_indices]
        X_val, y_val = X[validation_indices], y[validation_indices]

        perceptron = Perceptron(num_features=X.shape[1])
        perceptron.train(X_train, y_train)

        # Evaluate performance on validation set
        predictions = perceptron.predict(X_val)
        accuracy = np.mean(predictions == y_val)
        precision = np.sum((predictions == 1) & (y_val == 1)) / np.sum(predictions == 1)
        recall = np.sum((predictions == 1) & (y_val == 1)) / np.sum(y_val == 1)
        f1 = 2 * (precision * recall) / (precision + recall)

        print(f"Fold {fold+1} - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1_score: {f1:.2f}")

"""### Experiment - 1"""

import numpy as np
import matplotlib.pyplot as plt
data_input = np.load("inputs_Dataset-1.npy")
data_output = np.load("outputs_Dataset-1.npy")

data_input.shape

# Marking 0 in output to -1;
data_output[data_output==0] = -1
data_output.shape

X1 = data_input
y1 = data_output

# Initialize the Perceptron
perceptron = Perceptron(num_features=data_input.shape[1])

# Train the Perceptron
perceptron.train(X1, y1)

# Implementation of K-fold cross validation and calcuating performence matrix;

k_fold_cross_validation(X1, y1, k=5)

"""A) Report values of the performance metrics for different values of K."""

# a) Report values of the performance metrics for different values of K.

K_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]
for k in K_values:
    print(f"K = {k}")
    k_fold_cross_validation(X1, y1, k)
    print("=" * 40)

"""B) For each K report the mean and variances of the performance metrics
computed over all the folds.
"""

def compute_metrics_mean_variance(metrics_list):
    metrics_array = np.array(metrics_list)
    mean = np.mean(metrics_array, axis=0)
    variance = np.var(metrics_array, axis=0)
    return mean, variance

# b) For each K report the mean and variances of the performance metrics computed over all the folds.

def k_fold_variance (X, y, k):
    fold_size = len(X) // k           # floor division;
    indices = np.arange(len(X))
    np.random.shuffle(indices)

    accuracy_list = []
    precision_list = []
    recall_list = []
    f1_list = []

    for fold in range(k):
        # Seperating indices for training and validation using slicing of array;
        validation_indices = indices[fold * fold_size: (fold + 1) * fold_size]
        train_indices = np.concatenate([indices[:fold * fold_size], indices[(fold + 1) * fold_size:]])

        # Run the model ;
        X_train, y_train = X[train_indices], y[train_indices]
        X_val, y_val = X[validation_indices], y[validation_indices]

        perceptron = Perceptron(num_features=X.shape[1])
        perceptron.train(X_train, y_train)

        # Evaluate performance on validation set
        predictions = perceptron.predict(X_val)
        accuracy = np.mean(predictions == y_val)
        precision = np.sum((predictions == 1) & (y_val == 1)) / np.sum(predictions == 1)
        recall = np.sum((predictions == 1) & (y_val == 1)) / np.sum(y_val == 1)
        f1 = 2 * (precision * recall) / (precision + recall)

        accuracy_list.append(accuracy)
        precision_list.append(precision)
        recall_list.append(recall)
        f1_list.append(f1)

    accuracy_mean, accuracy_variance = compute_metrics_mean_variance(accuracy_list)
    precision_mean, precision_variance = compute_metrics_mean_variance(precision_list)
    recall_mean, recall_variance = compute_metrics_mean_variance(recall_list)
    f1_mean, f1_variance = compute_metrics_mean_variance(f1_list)

    print(f"K = {k}")
    print("Mean and Variance of Accuracy:", accuracy_mean, accuracy_variance)
    print("Mean and Variance of Precision:", precision_mean, precision_variance)
    print("Mean and Variance of Recall:", recall_mean, recall_variance)
    print("Mean and Variance of F1:", f1_mean, f1_variance)

K_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]

for k in K_values:
  k_fold_variance (X1, y1, k);
  print("=" * 80)

"""C) Make an 80:20 train-test split. For each iteration intraining, count the number
of misclassified instances.
 Plot iteration vs #misclassified instances.
"""

# c) Make an 80:20 train-test split. For each iteration intraining, count the number of misclassified instances.
# Plot iteration vsmisclassified instances.

def plot_misclassified_instances(X_train, y_train, max_epochs=1000):
    perceptron = Perceptron(num_features= X_train.shape[1])
    misclassified_count = []

    for epoch in range(max_epochs):
        misclassified = 0
        for i in range(len(X_train)):
            prediction = perceptron.predict(X_train[i])
            if prediction != y_train[i]:
                perceptron.weights += y_train[i] * X_train[i]
                perceptron.bias += y_train[i]
                misclassified += 1
        misclassified_count.append(misclassified)

    import matplotlib.pyplot as plt
    plt.plot(range(1, max_epochs + 1), misclassified_count)
    plt.xlabel('Iteration')
    plt.ylabel('Number of Misclassified Instances')
    plt.title('Iteration vs. Number of Misclassified Instances')
    plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

plot_misclassified_instances(X_train, y_train)

"""### Experiment 2"""

data_input = np.load("inputs_Dataset-2.npy")
data_output = np.load("outputs_Dataset-2.npy")

X2 = data_input
y2 = data_output

X2.shape

# Marking 0 in output to -1;
y2[y2==0] = -1
y2.shape

"""A) For each iteration intraining, count the number of misclassified instances. Plot iteration vs #misclassified instances."""

def plot_misclassified(X_train, y_train, max_epochs=2000):
    perceptron = Perceptron(num_features=X_train.shape[1])
    misclassified_count = []

    for epoch in range(max_epochs):
        misclassified = 0
        for i in range(len(X_train)):
            prediction = perceptron.predict(X_train[i])
            if prediction != y_train[i]:
                perceptron.weights += y_train[i] * X_train[i]
                perceptron.bias += y_train[i]
                misclassified += 1
        misclassified_count.append(misclassified)

    plt.plot(range(1, max_epochs + 1), misclassified_count)
    plt.xlabel('Iteration')
    plt.ylabel('Number of Misclassified Instances')
    plt.title('Iteration vs. Number of Misclassified Instances')
    plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.2, random_state=42)

plot_misclassified(X_train, y_train)

"""B) Report your observation on training with an explanation in one or two sentences.

As the Number of Misclassification Number is **Not** Converging. So, Dataset is **not linearly separable**.

C) Report values of the performance metrics for the test data.
"""

perceptron = Perceptron(num_features=X_train.shape[1])
perceptron.train(X_train, y_train)

test_predictions = perceptron.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, test_predictions)
train_predictions = perceptron.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)
precision = precision_score(y_test, test_predictions)
recall = recall_score(y_test, test_predictions)
f1 = f1_score(y_test, test_predictions)


print("Training Accuracy:", train_accuracy)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1:", f1)

"""### Experiment - 3"""

data_input = np.load("inputs_Dataset-3.npy")
data_output = np.load("outputs_Dataset-3.npy")

X3 = data_input
y3 = data_output

X3.shape

# Marking 0 in output to -1;
y2[y2==0] = -1
y2.shape

"""A) For each iteration intraining, count the number of misclassified instances. Plot iteration vs #misclassified instances."""

def plot_misclass(X_train, y_train, max_epochs=1000):
    perceptron = Perceptron(num_features=X_train.shape[1])
    misclassified_count = []

    for epoch in range(max_epochs):
        misclassified = 0
        for i in range(len(X_train)):
            prediction = perceptron.predict(X_train[i])
            if prediction != y_train[i]:
                perceptron.weights += y_train[i] * X_train[i]
                perceptron.bias += y_train[i]
                misclassified += 1
        misclassified_count.append(misclassified)

        if misclassified == 0:
            print(f"Converged in {epoch + 1} epochs.")
            break

    import matplotlib.pyplot as plt
    plt.plot(range(1, epoch + 2), misclassified_count)
    plt.xlabel('Iteration')
    plt.ylabel('Number of Misclassified Instances')
    plt.title('Iteration vs. Number of Misclassified Instances')
    plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X3, y3, test_size=0.2, random_state=42)

plot_misclass(X_train, y_train)

"""B) Report your observation on training with an explanation in one or two sentences.

Ans) As Number of Misclassifed instances converage after 150 iteration. Hence it is **Linearly Seperable Dataset**.

C) Report values of the performance metrics to the test data.
"""

perceptron = Perceptron(num_features=X_train.shape[1])
perceptron.train(X_train, y_train)

test_predictions = perceptron.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

train_predictions = perceptron.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)
accuracy = accuracy_score(y_test, test_predictions)
precision = precision_score(y_test, test_predictions)
recall = recall_score(y_test, test_predictions)
f1 = f1_score(y_test, test_predictions)

print("Training Accuracy:", train_accuracy)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1:", f1)

"""D) Connect Experiment 1, Experiment 2 and Experiment 3 by commenting on
the nature of the datasets.

Ans)

**Experiment 1 (Dataset-1):**
In Experiment 1, Dataset-1 exhibited convergence after approx **800** iteration and a significant reduction in misclassifications during training. This suggests that Dataset-1 is relatively well-linearly separable, enabling the Perceptron to quickly find an effective decision boundary. The high precision, recall, and F1 scores further emphasize the effectiveness of the model's predictions.

**Experiment 2 (Dataset-2):**
Experiment 2, does not give convergence in miss-qualified instance even after 2000 iteration. This implies that Dataset-2 might be non-linearity dataset. Despite the slower convergence, the model's predictions still achieved reasonable precision, recall, and F1 scores, although not as high as in Experiment 1.

**Experiment 3 (Dataset-3):**
In Experiment 3, Dataset-3 convereges rapidly as compared to both Experiment But fail in giving accuracy result. This implies that Dataset-3 might be nonlinear or noisy, making it difficult for a simple linear model like the Perceptron to find a suitable decision boundary. Consequently, the model's precision, recall, and F1 scores remained lower compared to the previous experiments.
"""

